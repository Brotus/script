\documentclass[a4paper,12pt,fleqn]{scrartcl}
 
%**** Benutzte Packages

% utf8 Zeichen verwendbar
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{enumerate}
%http://www.guntherkrauss.de/computer/tex/diagramme.html
\usepackage[arrow, matrix, curve]{xy}


%**** Selbst definierte Befehle

% Abkürzungen

% Äquivalenzpfeil
\newcommand{\gdw}{\Leftrightarrow}
% natürliche Zahlen
\newcommand{\N}{\mathbb{N}}
% reelle Zahlen
\newcommand{\R}{\mathbb{R}}
% kaligraphische Buchstaben
\newcommand{\m}[1]{\mathcal{ #1 }}
% Für Wahrscheinlichkeit, \prob{A}=P(A)
\newcommand{\prob}[1]{\text{P(} #1 \text{)}}
\newcommand{\p}[1]{\text{P(} #1 \text{)}}
% Abkürzungen
\newcommand{\ZE}{Zufallsexperiment}
\newcommand{\WR}{Wahrscheinlichkeitsraum}
\newcommand{\WM}{Wahrscheinlichkeitsmaß}
\newcommand{\Wk}{Wahrscheinlichkeit}
\newcommand{\ZV}{Zufallsvariable}
% Implikation
\newcommand{\impl}{\Rightarrow}
% geschlossenes reelles Einheitsintervall
\newcommand{\unit}{\left[ 0,1\right]}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\fa}[1]{\mathop{\forall}\limits_{#1}}
\newcommand{\ex}[1]{\mathop{\exists}\limits_{#1}}
\newcommand{\sk}{\mbox{}\\*}

%rowcounter zurücksetzen
\newcommand{\resetRows}{\setcounter{equation}{0}}

%**** Theoreme

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{satz}[definition]{Satz}
\newtheorem*{theorem}{Theorem}

\theoremstyle{plain}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{korollar}{Korollar}

\theoremstyle{remark}
\newtheorem*{bemerkung}{Bemerkung}
\newtheorem{beispiel}[definition]{Beispiel}
\newtheorem*{beispiel*}{Beispiel}

\begin{document}
\setcounter{section}{3}
\begin{titlepage}
\begin{center}
\textsc{\LARGE Einführung in die Stochastik}\\[2.0cm]
\rule{\linewidth}{0.5mm}
Eine Mitschrift der Vorlesung Einführung in die Stochastik von Prof. Dr. Kohler im Sommersemester 2016 an der TU Darmstadt.
\rule{\linewidth}{0.5mm}\\[2.0cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft}
\large \emph{Autoren:}\\\textsc{Johannes Struve\\Timo Seiche\\Julian Keinrath}\\[1.0cm]
\end{flushleft}
\end{minipage}
\vfill
Zuletzt bearbeitet am {\large \today}.
\end{center}
\end{titlepage}

% Inhaltsverzeichnis
\tableofcontents

% nächste Seite
\newpage

\section{Das mathematische Modell des Zufalls}
\textbf{Ziel:} Mathematische Modellierung \emph{zufälliger} Phänomene.\\
Ursachen für das Auftreten von Zufall:
\begin{itemize}
\item unvollständige Information
\item wurde zur Vereinfachung künstlich eingeführt
\end{itemize}
\subsection{Der Begriff der Wahrscheinlichkeit}
Ausgangspunkt: Zufallsexperiment (kurz: ZE)
\begin{definition}
Ein \emph{Zufallsexperiment} ist ein Experiment mit vorher unbestimmtem Ergebnis, das im Prinzip unbeeinflusst voneinander beliebig oft wiederholt werden kann.
\end{definition}
\begin{beispiel}
für \ZE:
\begin{itemize}
\item Werfen eines Würfels, Ergebnis ist Zahl oben
\item Wiederholtes Werfen eines Würfels, das Ergebnis ist die Zahl der Würfe, bis zum ersten Mal $6$ gewürfelt wird.
\end{itemize}
kein \ZE:
\begin{itemize}
\item Nächste Bundestagswahl (kann nicht unbeeinflusst voneinander wiederholt werden)
\end{itemize}
\end{beispiel}
Ziel: Aussagen über Ergebnisse von \ZE en:
\begin{definition}
\begin{itemize}
\item Die Menge aller möglichen Ergebnisse des \ZE s wird als \emph{Grundmenge} $\Omega$ bezeichnet.
\item Jede Teilmenge der Grundmenge heißt \emph{Ereignis}. Die einelementigen Teilmengen der Grundmenge heißen \emph{Elementarereignis}.
\item Ein Ereignis $A$ \emph{tritt ein}, falls das Ergebnis des Zufallsexperiments in $A$ liegt. Andernfalls tritt $A$ nicht ein.
\end{itemize}
\end{definition}
\begin{beispiel}
Werfen eines echten Würfels. Das Ergebnis ist die Zahl oben.
\[\impl\Omega=\{ 1,2,3,4,5,6\}\]
Das Ereignis $A=\{ 2,4,6\}$ tritt genau dann ein, wenn eine gerade Zahl gewürfelt wird.
\end{beispiel}
\begin{definition}
\ZE \, mit Grundmenge $\Omega$. Sind $x_1,\ldots,x_n\in\Omega$ die bei wiederholtem Durchführen des \ZE s auftretenden Werte, so heißen
\[\mid \{ 1\leq i\leq n:x_i\in A\}\mid\]
bzw.
\[\frac{\mid \{ 1\leq i\leq n:x_i\in A\}\mid}{n}\]
die \emph{absolute} bzw. \emph{relative Häufigkeit} des Eintretens von $A$.
\end{definition}
\begin{bemerkung}[Empirisches Gesetz der großen Zahlen]
Führt man ein \ZE \, unbeeinflusst voneinander immer wieder durch, so nähert sich für große Anzahlen von Wiederholungen die relative Häufigkeit des Eintretens eines beliebigen Ereignisses $A$ einer Zahl
\[\prob{A} \in\unit\]
an. Wir bezeichnen $\prob{A}$ als \emph{Wahrscheinlichkeit} von $A$.
\end{bemerkung}
\begin{beispiel}
Werfen eines echten Würfels, das Ergebnis ist die Zahl oben.\\
Relative Häufigkeiten der Elementarereignisse nähern sich immer mehr $1/6$ an.
\[\prob{\{ 1\}}=\ldots=\prob{\{ 6\} }=\frac{1}{6}\]
\end{beispiel}
Ziel im Weiteren: Bestimmung von Wahrscheinlichkeiten ohne wiederholte Durchführung des \ZE s. Manchmal ist die \emph{Kombinatorik} hilfreich dabei.

\subsection{Grundlagen der Kombinatorik}
Ziehen von $k$ Elementen aus einer Menge mit Mächtigkeit $n$.\\
\\
Hier werden 4 Möglichkeiten betrachtet:
\begin{itemize}
\item Ziehen mit bzw. ohne Zurücklegen
\item Ziehen mit bzw. ohne Beachtung der Reihenfolge
\end{itemize}
Sei $N$ die Anzahl der möglichen Stichproben. Dann gilt...
\begin{enumerate}[a)]
\item ...beim \emph{Ziehen mit Zurücklegen und mit Beachtung der Reihenfolge}:
\[N = n \cdot n \cdot \ldots \cdot n = n^{k}\]
\item ...beim \emph{Ziehen ohne Zurücklegen und mit Beachtung der Reihenfolge}:
\[N = n \cdot (n-1) \cdot \ldots \cdot (n-k+1) = \frac{n!}{(n-k)!}\]
\item ...beim \emph{Ziehen ohne Zurücklegen und ohne Beachtung der Reihenfolge}:\\
Ordnet man jede der Stichproben auf alle $k!$ möglichen Arten (vgl. b)), so erhält man alle Stichproben beim Ziehen ohne Zurücklegen und mit Beachtung der Reihenfolge.
\[\impl N \cdot k! = \frac{n!}{(n-k)!}\]
\[\impl N = \frac{n!}{(n-k)! \cdot k!} = \binom{n}{k}\]
\item ...beim \emph{Ziehen mit Zurücklegen und ohne Beachtung der Reihenfolge}:
\[N = \binom{n+k-1}{k}\]
Begründung: Gesucht ist $\mid \Omega \mid$, wobei 
\[\Omega=\{(x_1, \ldots, x_k) \in \N^{k}:1 \leq x_1 \leq x_2 \leq \ldots \leq x_k \leq n\}.\]
Setze \[\Omega^\prime=\{(y_1, \ldots, y_k) \in \N^{k}:1 \leq y_1 < y_2 < \ldots < y_k \leq n+k-1\}\] und definiere $f: \Omega \rightarrow \Omega^\prime$ durch $f((x_1, x_2, x_3, \ldots, x_k)) = (x_1, x_2+1, x_3+2, \ldots, x_k+k-1)$. Zu zeigen ist, dass $f$ eine (wohldefinierte) bijektive Abbildung ist. Dazu:
\begin{enumerate}[(i)]
\item $f$ ist wohldefiniert, denn $1 \leq x_1 \leq \ldots \leq x_k \impl 1 \leq x_1 < \ldots < x_k+k-1$.
\item $f$ ist injektiv:
\begin{align*}
&f((x_1, x_2, \ldots, x_k)) = f((\widetilde{x_1}, \widetilde{x_2}, \ldots, \widetilde{x_k})) \\
\impl & (x_1, x_2+1, \ldots, x_k+k-1) = (\widetilde{x_1}, \widetilde{x_2}+1, \ldots, \widetilde{x_k}+k-1) \\
\impl & (x_1, x_2, \ldots, x_k) = (\widetilde{x_1}, \widetilde{x_2}, \ldots, \widetilde{x_k})
\end{align*}
\item $f$ ist surjektiv:
\begin{align*}
&(y_1, y_2, \ldots, y_k) \in \Omega^\prime \\
\impl & 1 \leq y_1 < y_2 <\ldots < y_k \leq n+k-1 \\
\impl & 1 \leq y_1 \leq y_2-1 \leq \ldots \leq y_k-k+1 \leq n \\
\impl & (y_1,y_2-1, \ldots , y_k-k+1) \in \Omega
\end{align*}
Dann ist $\widetilde{y} \in \Omega$ und $f(\widetilde{y})=(y_1, \ldots, y_k) \in \Omega^\prime$. \\
$\impl \mid \Omega \mid = \mid \Omega^\prime \mid = \binom{n+k-1}{k}$
\end{enumerate}
\end{enumerate}
\begin{beispiel}[Binomischer Lehrsatz]
Für $a, b \in \R, n \in \N$ gilt 
\[(a+b)^{n} = \sum_{k=0}^{n} \binom{n}{k} a^{k} b^{n-k}.\]
Begründung:
\[(a+b)^{n} = \sum_{k=0}^{n} n_k a^{k} b^{n-k}\]
Hier bezeichnet $n_k$ die Anzahl der Mögichkeiten, um aus $n$ Faktoren $k$ auszuwählen (bei denen $a$ gewählt wird) ohne Zurücklegen und ohne Beachtung der Reihenfolge. Ohne Zurücklegen, da kein Faktor doppelt gewählt werden kann; ohne Beachtung der Reihenfolge, da Reihenfolge beim Produkt egal ist. Aus c) folgt $n_k = \binom{n}{k}$.
\end{beispiel}

\subsection{Drei Beispiele}
\begin{beispiel}
Ein (echter) Würfel wird solange geworfen, bis er zum ersten Mal mit $6$ oben landet. Wie groß ist die Wahrscheinlichkeit, dass die Anzahl der Würfe gerade ist?\\
Das Ergebnis des \ZE s ist die Zahl der Würfe bis zur ersten $6$, wobei der letzte Wurf mitgezählt wird.\\
Hier ist
\[\Omega = \{1,2,3,\ldots\} \cup \{\infty\}\]
und gesucht ist die Wahrscheinlichkeit des Ereignisses
\[A=\{2,4,6,8,\ldots\}.\]
Ansatz: $\prob{A} = \sum_{k \in A} \prob{\{k\}}$ \\
Die Wahrscheinlichkeit eines Ereignisses ist die Summe der Wahrscheinlichkeiten aller darin enthaltenen Elementarereignisse.\\
\\
Bestimmung von $\prob{\{k\}}$ für $k \in \N$: \\
Stimmt mit Wahrscheinlichkeit überein, dass beim k-maligen Werfen eines echten Würfels (Ziehen von $k$ Elementen aus einer Grundmenge von Umfang $6$ mit Zurücklegen, mit Beachtung der Reihenfolge)\\
Von diesen erscheint bei genau
\[5^{k-1} \cdot 1 = 5^{k-1}\]
beim $k$-ten Wurf die erste $6$. Jedes der $6^{k}$ Elementarereignisse hat (wegen der Symmetrie) die gleiche Wahrscheinlichkeit von $6^{-k}$.
\[\impl \prob{\{k\}}=\frac{5^{k-1}}{6^{k}}=\frac{1}{6}(\frac{5}{6})^{k-1}\]
Damit ist
\begin{align*}
\prob{A}&=\sum_{k \in A} \prob{\{k\}}=\frac{1}{6}\sum_{k \in A}(\frac{5}{6})^{k-1}=\frac{1}{6}\sum_{i=1}^{\infty}(\frac{5}{6})^{2i-1}=\frac{1}{6}\sum_{i=1}^{\infty}(\frac{5}{6})^{2(i-1)+1} \\
&=\frac{1}{6}\cdot\frac{5}{6}\sum_{i=0}^{\infty}((\frac{5}{6})^{2})^{i}=\frac{5}{36}\sum_{i=1}^{\infty}(\frac{25}{36})^{i}=\frac{5}{36}\cdot\frac{1}{1-\frac{25}{36}}=\frac{5}{36-25} \\
&=\frac{5}{11}.
\end{align*}
\end{beispiel}
\begin{beispiel}
Jackpot beim Lotto \glqq6 aus 49\grqq \, im Dezember 2007: 43 Mio. Euro \\
Beobachtung: In den $4599$ Ziehungen von Oktober 1955 bis Dezember 2007 wurde die $38$ am häufigsten gezogen, nämlich $614$-mal. \\
Zum Vergleich
\[\frac{4599 \cdot 6}{49} = 563\]
Frage: Zufall? \\
\underline{Grundidee in der Stochastik zur Beantwortung dieser Frage:}
\begin{itemize}
\item Nimm an, dass die $6$ Zahlen zufällig gezogen werden, d.h. jede Zahlenkombination tritt mit gleicher Wahrscheinlichkeit auf.
\item Berechne unter dieser Annahme Wahrscheinlichkeit, dass das Resultat beobachtet wird, welches so stark gegen die Annahme spricht, wie das tatsächlich beobachtete.
\item Falls Wahrscheinlichkeit klein ist (z.B. $\leq 0.05$), verwirf die Annahme, andernfalls nicht.
\end{itemize}
Im Folgenden: Bestimmen der Wahrscheinlichkeit, dass bei $n = 4599$ Ziehungen die $38$ mindestens $614$-mal gezogen wird. \\
Dazu:
\begin{align*}
&\prob{\text{Bei } n \text{ Ziehungen } 38 \text{ mindestens } 614 \text{-mal}}\\
= &\sum_{k = 614}^{n} (\prob{\text{Bei } n \text{ Ziehungen } 38 \text{ genau } k \text{-mal}})
\end{align*}
Nun gilt: \\
Wahrscheinlichkeit, dass $38$ bei einer Ziehung gezogen wird
\begin{align*}
&= \frac{ \text{\# günstigste Fälle}}{\text{\# mögliche Fälle}} \\
&= \frac{1 \cdot \binom{48}{5}}{\binom{49}{6}} \\
&= \frac{\frac{48!}{(48-5)! \cdot 5!}}{\frac{49!}{(49-6)!}} = \frac{6}{49} =: p
\end{align*}
(Obere Vorgehensweise benutzt ohne Zurücklegen und ohne Beachtung der Reihenfolge, in diesem Fall ist es aber auch möglich mit Beachtung der Reihenfolge zu betrachten.) \\

Wahrscheinlichkeit, dass $38$ bei $n = 4599$ aufeinanderfolgenden Ziehungen genau $k$-mal auftritt.
\begin{align*}
&= \frac{\text{\# günstige Fälle}}{\text{\# mögliche Fälle}} \\
&= \frac{\binom{n}{k} \cdot \binom{48}{5}^k \cdot (\binom{49}{6} - \binom{48}{5})^{(n-k)} }{\binom{49}{6}^n}
\end{align*}
Anmerkungen zu den Faktoren: \\ 
$\binom{n}{k}$ = Position der $k$ Ziehungen \\
$\binom{48}{5}^k$ = $k$ Ziehungen mit $38$ \newpage
\begin{align*}
(\binom{49}{6} - \binom{48}{5})^{(n-k)} &= n-k \text{ Ziehungen ohne } 38 \\
&= \binom{n}{k} \cdot (\frac{\binom{48}{5}}{\binom{49}{6}})^k \cdot (1 - \frac{\binom{48}{5}}{\binom{49}{6}})^{(n-k)} \\
&= \binom{n}{k} \cdot p^k \cdot (1-p)^{(n-k)} \\
\impl &\prob{\text{Bei } n \text{ Ziehungen } 38 \text{ mindestens } 614\text{-mal}} \\
&=\sum_{k=614}^n (\binom{n}{k} \cdot p^k \cdot (1-p)^{(n-k)}) \approx 0.01
\end{align*}
Dies ist sehr klein. Aber es spricht eigentlich jedes Resultat gegen die Annahme, bei dem irgendeine Zahl mindesten $614$-mal auftritt. \\
Die Wahrscheinlichkeit, dass irgendeine der $49$ Zahlen bei $n=4599$ Ziehungen mindestens $614$-mal gezogen wird (per Computersimulation) 
\[\approx 0.47,\]
was nicht klein ist! \\
Fazit: Annahme kann nicht verworfen werden.
\end{beispiel}

% Ab hier 9.5.
\begin{beispiel}
Eine Zahl wird rein zufällig aus $\left[ 0,5\right]$ gezogen. Wie groß ist die Wahrscheinlichkeit, dass sie zwischen $2$ und $4$ liegt?  Hier ist $\Omega=\left[ 0,5\right]$, gesucht ist die Wahrscheinlichkeit von $A=\left[ 2,4\right]$. Da alle Teile von $\Omega$ \glqq gleichberechtigt\grqq \, sind, machen wir den Ansatz
\[\prob{\left[ a,b\right]}.\]
Es folgt
\[\prob{\left[ 2,4\right]}=\frac{4-2}{5}=0.4.\]
\begin{bemerkung}
Hier ist
\[\prob{A}=\sum_{\omega\in A}\prob{\{\omega \} }\]
nicht möglich, da $\prob{\{ x\} } = 0$ für alle $x\in\Omega$.
\end{bemerkung}
\end{beispiel}
\begin{bemerkung}[Problem]
Ansätze unsystematisch! Gibt es auch systematischen Zugang?
\end{bemerkung}

\subsection{Der Begriff des Wahrscheinlichkeitsraumes}
\begin{definition}
Sei $\Omega\neq\emptyset$. Eine Menge $\m{A}\subset\m{P}(\Omega)$  heißt $\sigma$-Algebra (über $\Omega$), falls gilt
\begin{enumerate}
\item $\emptyset\in\m{A}$
\item $A\in\m{A}\impl \Omega\setminus A\in\m{A}$
\item $(A_n)_{n\in\N}\impl\bigcup_{n\in\N}A_n$
\end{enumerate}
\end{definition}
\begin{beispiel}
\begin{enumerate}[a)]
\item $\m{A}=\m{P}(\Omega)$
\item $\m{A}=\{\emptyset,\Omega\}$
\item Die kleinste $\sigma$-Algebra über $\R$, die alle Intervalle enthält, ist die sogenannte \emph{Borelsche $\sigma$-Algebra} $\m{B}$.
\end{enumerate}
\end{beispiel}
\begin{bemerkung}[Zur Existenz von $\m{B}$]
Sei $I\neq\emptyset$ eine Indexmenge, $\m{A}_i$ $\sigma$-Algebra über $\Omega$ für $i\in I$ impliziert $\bigcap_{i\in I}\m{A}_i$ ist $\sigma$-Algebra über $\Omega$. Setze dann:
\[\m{B}:=\bigcap\{\m{A}|\m{A}\text{ ist }\sigma\text{-Algebra über }\R\text{ und enthält alle Intervalle}\}\]
$\m{B}$ ist nicht gleich der Potenzmenge von $\R$.
\end{bemerkung}
\begin{definition}
Sei $\Omega\neq\emptyset,\m{A} \, \sigma$-Algebra über $\Omega$. Jede Abbildung $\text{P}:\m{A}\to\R$ mit
\begin{enumerate}[i.]
\item $\fa{A\in\m{A}}:\prob{A}\in\left[ 0,1\right]$,
\item $\prob{\emptyset}=0,\prob{\Omega}=1$,
\item $A\in\m{A}\impl \prob{A^C}=1-\prob{A}$,
\item $A,B\in\m{A},A\subset B\impl \prob{A}\leq \prob{B}$,
\item $(A_i)_{i\in I}$, $I$ abzählbar, $A_i$ paarweise disjunkt: $\prob{\bigcup_{i\in I}} A_i=\sum_{i\in I}\prob{A_k}$
\end{enumerate}
heißt \emph{Wahrscheinlichkeitsmaß} (kurz: W-Maß).
\end{definition}
\begin{lemma}
$\text{P}:\m{A}\to\R$ ist ein \WM, genau dann wenn:
\begin{enumerate}
\item $\p{A}\geq 0$
\item $\p{\Omega}=1$
\item $\sigma$-Additivität
\end{enumerate}
\end{lemma}
\begin{proof}
\grqq $\impl$\grqq: trivial. \\
\grqq $\Leftarrow$\grqq: Es gilt
\begin{itemize}
\item $\p{\emptyset}=\p{\bigcup_{n\in\N}\emptyset}=\sum_{n\in\N}\p{\emptyset}\impl\p{\emptyset}=0$
\item $\p{ A\cup B}=\p{ A\cup B\cup\emptyset\ldots}=\p{ A}+\p{ B}$
\item $A\subseteq B\impl B=A\cup (B\setminus A)\impl\p{ B}=\p{A}+\p{B\setminus A}\geq\p{A}$
\item $1=\p{\Omega}=\p{A}+\p{\Omega\setminus A}\impl\p{A^C}=1-\p{A}$
\end{itemize}
\end{proof}
\begin{definition}
Ist $\Omega\neq\emptyset$, $\m{A}$ $\sigma$-Algebra über $\Omega$ und $\text{P}:\m{A}\to\R$ ein \WM, so heißt $(\Omega,\m{A},\text{P})$ \emph{Wahrscheinlichkeitsraum} (kurz: W-Raum). In diesem Fall heißt $\p{A}$ \emph{Wahrscheinlichkeit} (kurz: Wk).
\end{definition}

% 13.05.2016
\begin{lemma}
Sei $(\Omega, \m{A}, P)$ ein \WR. Dann gilt
\begin{enumerate}[a)]
\item $A, B \in \m{A}, A \subseteq B \impl \p{B \backslash A} = \p{B} - \p{A}$
\item $A_1, A_2, \ldots , A_n \in \m{A} \impl \p{\bigcup_{k=1}^n A_k} \leq \sum_{k=1}^n \p{A_k}$ und $\p{\bigcup_{k=1}^\infty A_k} \leq \sum_{k=1}^\infty \p{A_k}$ (sogenannte $\sigma$-Subadditivität)
\item $A, B \in \m{A} \impl \p{A \cup B} = \p{A} + \p{B} - \p{A \cap B}$
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}[a)]
\item Seien $A, B \in \m{A}$ mit $A \subseteq B$.
\begin{align*}
\impl &B \backslash A, A \in \m{A} \text{ disjunkt, und } B = (B \backslash A) \cup A \\
\impl &\p{B} = \p{(B \backslash A) \cup A} = \p{B \backslash A} + \p{A}, \\
&\text{ da } P \text{ ein \WM \, ist} \\
\impl &\p{B \backslash A} = \p{B} - \p{A}
\end{align*}
\item $\bigcup_{k=1}^n A_k = A_1 \cup \bigcup_{k=1}^n (A_k \backslash (\bigcup_{l=1}^{k-1} A_l))$, wobei $A_1, A_k \backslash (\bigcup_{l=1}^{k-1} A_l) \in \m{A}$ gilt, da $\m{A}$ eine $\sigma$-Algebra ist und beide paarweise disjunkt sind. Da $P$ ein \WM \, ist, gilt
\begin{align*}
\p{\bigcup_{k=1}^n A_k} &= \p{A_1} + \sum_{k=2}^n \p{A_k \backslash (\bigcup_{l=1}^{k-1} A_l)} \\
&\leq \p{A_1} + \sum_{k=2}^n \p{A_k},
\end{align*}
da $A_k \backslash (\bigcup_{l=1}^{k-1} A_l) \subseteq A_k.$ Der zweite Teil funktioniert analog.
\item Es gilt $A \cup B = (A \backslash (A \cap B)) \cup (B \backslash (A \cap B)) \cup (A \cap B)$, wobei $A \backslash (A \cap B), B \backslash (A \cap B), A \cap B \in \m{A}$ gilt, da $\m{A}$ eine $\sigma$-Algebra ist. Zudem sind sie paarweise disjunkt. Da $P$ ein \WM \, ist, gilt
\begin{align*}
\p{A \cup B} &= \p{A \backslash (A \cap B)} + \p{B \backslash (A \cap B)} + \p{A \cap B} \\
&= \p{A} - \p{A \cap B} + \p{B} - \p{A \cap B} + \p{A \cap B} \\
&= \p{A} + \p{B} - \p{A \cap B}
\end{align*}
\end{enumerate}
\end{proof}
\begin{lemma*}[von Borel-Cantelli]
Sei $(\Omega, \m{A}, P)$ ein \WR, $A_n \in \m{A} \, (n \in \N)$ und 
\[\limsup A_n := \bigcap_{k=1}^\infty \bigcup_{k=n}^\infty A_k.\]
Aus $\sum_{k=1}^\infty \p{A_n} < \infty$ folgt $\p{\limsup A_n} = 0$.
\end{lemma*}
\begin{bemerkung}
Es ist einfach zu sehen, dass
\[\limsup A_n = \{\omega \in \Omega : \text{Für unendlich viele } n \text{ gilt } \omega \in A_n\}\]
\end{bemerkung}
\begin{beispiel*}
Ein Student S macht bei $n$-ter Aufgabe einen Fehler mit \Wk \, $\frac{1}{n^2}$. Dann gilt
\begin{align*}
&\p{\text{\glqq Student S macht unendlich viele Fehler \grqq}} \\
= \, &\p{\limsup A_n} \text{ mit } A_n = \text{\glqq Fehler bei } n \text{-ter Aufgabe \grqq} \\
= \, &0 \text{, da } \sum_{k=1}^n \p{A_k} = \sum_{k=1}^\infty \frac{1}{k^2} < \infty.
\end{align*}
\end{beispiel*}
\begin{proof}[Beweis des Lemmas von Borel-Cantelli]
Es gilt $limsup A_n = \bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_k \subseteq \bigcup_{k=N}^\infty A_k$ für jedes $N \in \N$. Daraus folgt
\begin{align*}
&\p{\limsup A_n} \leq \p{\bigcup_{k=n}^\infty A_k} \leq \sum_{k=n}^\infty \p{A_k} \stackrel{\N \rightarrow \infty}{\rightarrow} 0 \text{, da } \sum_{k=1}^\infty \p{A_k} < \infty. \\
\impl \, &\p{\limsup A_n} \leq 0 \\
\impl \, &\p{\limsup A_n} = 0,
\end{align*}
da $\p{A} \geq 0$ für alle $A \in \m{A}$, da $P$ ein \WM \, ist.
\end{proof}
\subsection{Der Laplacesche \WR}
Wir betrachten \ZE e, bei denen:
\begin{enumerate}[a)]
\item nur endlich viele verschiedene Werte als mögliches Ergebnis vorkommen
\item jeder dieser Werte mit gleicher Wahrscheinlichkeit auftritt.
\end{enumerate}
\begin{satz}
Sei $\Omega \neq \emptyset$ endlich und $P: \m{P}(\Omega) \rightarrow \R$ definiert durch
\[\p{A} = \frac{\mid A \mid}{\mid \Omega \mid} \quad (A \in \m{P}(\Omega)).\]
Dann ist $(\Omega, \m{P}(\Omega), P)$ ein \WR. In diesem gilt
\[\p{\{\omega\}} = \frac{1}{\mid \Omega \mid} \quad (\omega \in \Omega).\]
\end{satz}
\begin{proof}
Nach Lemma 4.14 genügt es, Folgendes zu zeigen:
\begin{enumerate}[(1)]
\item $\p{A} \geq 0 \, (A \in \m{P}(\Omega))$
\item $\p{\Omega} = 1$
\item $P$ ist $\sigma$-additiv
\end{enumerate}
Dazu:
\begin{enumerate}[(1)]
\item ist klar.
\item $\p{\Omega} = \frac{\mid \Omega \mid}{\mid \Omega \mid} = 1$
\item Seien $A_1, A_2, \ldots, A_n \in \m{P}(\Omega)$ paarweise disjunkt. Da $\Omega$ endlich ist, gilt $A_i = \emptyset$ für alle $i > k$ für ein geeignetes $k \in \N$. Daraus folgt
\begin{align*}
\p{\bigcup_{n=1}^\infty A_n} &= \p{\bigcup_{n=1}^k A_n} = \frac{\mid \bigcup_{n=1}^k A_n \mid}{\mid \Omega \mid} = \frac{\sum_{n=1}^k \mid A_n \mid}{\mid \Omega \mid} \\
&= \sum_{n=1}^k \frac{\mid A_n \mid}{\mid \Omega \mid} = \sum_{n=1}^k \p{A_n} = \sum_{n=1}^\infty \p{A_n},
\end{align*}
da $A_i = \emptyset$ für $i > k$ und $\p{\emptyset} = 0$ ist.
\end{enumerate}
\end{proof}
\begin{definition}
Der \WR \, in Satz 4.17 heißt \emph{Laplacescher \WR}.
\end{definition}
\begin{bemerkung}
Im Laplaceschen \WR \, gilt immer
\[\p{A} = \frac{\mid A \mid}{\mid \Omega \mid} = \frac{\text{\# für } A \text{ günstige Fälle}}{\text{\# mögliche Fälle}}.\]
\end{bemerkung}

% 20.5. Julian
\begin{beispiel}
Eine echte Münze wird viermal unbeeinflusst voneinander geworfen. Wie groß ist die Wahrscheinlichkeit, dass die Münze mindestens einmal mit Kopf oben landet? \\
Wir betrachten das \ZE \, durch Laplaceschen \WR \, $( \Omega , \m{P}(\Omega)  , P)$ mit 
\begin{align*}
\Omega = \{ (w_1, w_2, w_3, w_4) : w_i \in \{ K , Z \} \, (i = 1,...,4) \} 
\end{align*} 
und 
\begin{align*}
 \p{A} = \frac{|A|}{|\Omega|} = \frac{|A|}{16}
\end{align*}
Gesucht: \Wk \, von  $B = \{ (w_1,...,w_4) \in \Omega : w_i = K \} $ für mindestens ein $ i \in \{1,...,4 \} $. \\
Mit 
\[B^c = \{ (Z,Z,Z,Z) \} \]
folgt
\[\p{B} = 1- \p{B^c} = 1- \frac{|B^c|}{16} = 1- \frac{1}{16} = \frac{15}{16}\]
\end{beispiel}

\subsection{Wahrscheinlichkeitsräume mit Zähldichte}

Im Folgenden: $\Omega$ endlich oder abzählbar unendlich. \\
Wegen der $\sigma$-Additivität gilt:
\[A = \bigcup_{w \in A} \{w\} \impl \p{A} = \sum_{w \in A} \p{\{w\}}\]
\WM e sind eindeutig bestimmt durch die \Wk \, der Elementarereignisse.

\begin{satz}
Sei $\Omega = \{ x_1,x_2,... \}$ eine abzählbar unendliche Menge mit paarweise verschiedenen Elementen, $(p_k)_{k \in N}$ eine Folge in $\R$ mit $0 \leq p_k \leq 1$ und
\[\sum_{k=1}^{\infty}{p_k} = 1.\]	 
Dann wird durch $( \Omega , \m{P}(\Omega)  , P)$ mit 
\[\p{A} = \sum_{k:x_k \in A}{p_k}\]
ein \WR \, definiert mit $\p{x_k} = p_k.$
\end{satz}
\begin{proof}
\begin{itemize}
\item wohldefiniert, da unendliche Summen nicht-negativer Zahlen nicht von Summationsreihenfolge abhängen.
\item $\p{A} \geq 0$ da $p_k \geq 0$
\item $\p{ \Omega } = \sum_{k:x_k \in A}{p_k} = \sum_{k=1}^{\infty}{p_k} = 1$
\item Seien $A_1, A_2,... \subset \Omega $ paarweise disjunkt. Wir zeigen $ \p{ \bigcup_{k=1}^{ \infty }{A_k} } = \sum_{k=1}^{\infty}{\p{A_k}}$.\\ 
Auf beiden Seiten werden die gleichen Zahlen aufaddiert, nur in anderer Reihenfolge. Da diese bei Summen nichtnegativer Zahlen keine Rolle spielt, folgt Gleichheit. \\
Mit Lemma 4.14 folgt, dass P ein \WM \, ist.
\end{itemize}
\end{proof}
\begin{bemerkung}
Satz 4.20 gilt analog auch im Endlichen.
\end{bemerkung}
\begin{definition}
Die Folge $(p_k)_{k \in \N}$ heißt Zähldichte des \WM es in Satz 4.20.
\end{definition}
\begin{beispiel}
Bei einer Umfrage werden $n$ Personen rein zufällig ausgewählt und gefragt, welche Partei sie nächsten Sonntag wählen würden. Wie groß ist die \Wk , dass der relative Anteil an SPD-Wählern unter den Befragten um nicht mehr als 1 \% vom entsprechenden Anteil $p$ in der gesamten Bevölkerung abweicht? \\
Vereinfachung: Der Anteil SPD-Wähler ändert sich nach Herausgreifen eines Befragten nicht. \\ \\
Sei $N$ die Anzahl der befragten Personen. Die \Wk , dass genau $k$ SPD-Wähler unter $n$ Befragten sind ist
\[\frac{\text{\# günstige Fälle}}{\text{\# mögliche Fälle}} = \frac{\binom{n}{k} (Np)^k (N-Np)^{n-k}}{N^n}\]
Wegen $|\frac{k}{n} - p | \leq 0.01 \iff np-n0,01 \leq np + n0,01$ ist die gesuchte \Wk \, gleich :
\begin{align*}
&\p{ \{ k \in \{ 0,1, \ldots ,n \} : |\frac{k}{n} -p| \leq 0,01 \}} \\
=& \sum_{k \in \{ 0,1, \ldots ,n \}: np-n0.01 \leq k \leq np+n0.01}{  \binom{n}{k} p^k (1-p)^{n-k}}.
\end{align*}
Die \Wk \, hängt also nicht von $N$ ab.
\end{beispiel}

% 23.5. Johannes
\begin{definition}
Sei $n\in\N,p\in\left[ 0,1\right]$. Das gemäß Satz 4.20 durch $\Omega=\N_0$ und die Zähldichte $(b(n,p,k))_{k\in\N_0}$ mit
\[b(n,p,k)=\begin{cases}
\binom{n}{k}p^k(1-p)^{n-k}&,k\in\{ 0,1,\ldots,n\}\\
0&,sonst
\end{cases}\]
festgelegte \WM \, heißt \emph{Binomialverteilung} mit Parametern $n$ und $p$.
\end{definition}
\begin{bemerkung}
Zähldichte, da
\[1=(p+(1-p))^n=\sum_{k=0}^n\binom{n}{k}p^k(1-p)^{n-k}.\]
\end{bemerkung}
\begin{lemma}
Für $p_n\in\left[ 0,1\right]$ mit $n\cdot p_n\to\lambda\in\R_{\geq 0}$ gilt für jedes $k\in\N_0$:
\[\binom{n}{k}p_n^k(1-p_n)^{n-k}\to\frac{\lambda^k}{k!}e^{-\lambda}.\]
\end{lemma}
\begin{proof}
Es gilt:
\begin{align}
p_n=&\frac{n\cdot p_n}{n}\to 0\\
\impl &\binom{n}{k}p_n^k(1-p_n)^{n-k}\\
=&\frac{n(n-1)\cdot\ldots\cdot(n-k+1)}{k!}p_n^k(1-p_n)^{n-k}\\
=&\frac{1}{k!}np_n(np_n-p_n)\ldots(np_n-(k-1)p_n)(1-p_n)^{-k}(1-p_n)^n\\ 
=&\frac{1}{k!}np_n(np_n-p_n)\ldots(np_n-(k-1)p_n)(1-p_n)^{-k}\exp(np_n\frac{\log(1-p_n)}{p_n})\\ 
&\rightarrow\frac{\lambda^k}{k!}e^{-\lambda}
\end{align}
(Die mittleren Faktoren in $(4)$ konvergieren jeweils gegen $\lambda$ und mit de l'Hopital folgt die Konvergenz zu $\exp(-\lambda)$)
\end{proof}
\begin{definition}
Sei $\lambda>0$. Das gemäß Satz 4.20 durch $\Omega=\N_0$ und die Zähldichte
\[\pi(\lambda;k)=\frac{\lambda^k}{n!}e^{-\lambda}\]
für $k\in\N_0$ festgelegte \WM \, heißt \emph{Poisson-Verteilung} mit Parameter $\lambda$.
\end{definition}
\begin{bemerkung}
Zähldichte, da
\begin{align*}
\sum_{k\in\N_0}\frac{\lambda^k}{k!}e^{-\lambda}=e^{-\lambda}\sum_{k\in\N_0}\frac{\lambda^k}{k!}=1.
\end{align*}
\end{bemerkung}
\subsection{Wahrscheinlichkeitsräume mit Dichten}
In diesem Abschnitt: $\Omega$ überabzählbar.
Hierbei ist
\[\prob{A}=\sum_{\omega\in A}\prob{\{\omega\}}\]
nicht sinnvoll, da eventuell über überabzählbare Mengen summiert wird oder die $\prob{\{\omega\}}=0$ für alle $\omega\in\Omega$, vgl. Bsp. 4.10. Deswegen wird die Summe durch ein Integral approximiert.
\begin{satz}
Ist $f:\R\to\R$ eine Funktion mit $f\geq 0$ und 
\begin{equation*}\label{eq:4.2}
\int_{\R}f\mathrm{d}x=1,
\end{equation*}
so wird durch $(\Omega,\m{A},\text{P})$ mit $\Omega=\R$, $\m{A}=\m{B}$ und
\begin{equation*}\label{eq:4.3}
\prob{B}=\int_{B}f(x)\mathrm{d}x,(B\in\m{B})
\end{equation*}
ein \WR \, definiert.
\end{satz}
\begin{proof}
Wegen \eqref{eq:4.2} gilt:
\[\prob{B}=\int_Bf(x)\mathrm{d}x\geq 0\]
und
\[\prob{\Omega}=\int_{\R}f(x)\mathrm{d}x=1.\]
Nach Lemma 4.14 genügt es daher zu zeigen: Sind $A_1,A_2,...\in \m{B}$ paarweise disjunkt, so gilt
\begin{align*}
\sum_{n\in\N}\prob{A_n}=&\sum_{n\in\N}\int_{A_n}f(x)\mathrm{d}x\\
=&\sum_{n\in\N}\int_{\R}f(x)\m{X}_{A_n}(x)\mathrm{d}x\\
=&\int_{\R}\sum_{n\in\N}f(x)\cdot\m{X}_{A_n}(x)\mathrm{d}x\\
=&\int_{\R}f(x)\cdot\sum_{n\in\N}\m{X}_{A_n}(x)\mathrm{d}x\\
=&\int_{\R}f(x)\cdot\m{X}_{\bigcup_{n\in\N}A_n}(x)\mathrm{d}x\\
=&\int_{\bigcup_{n\in\N}A_n}f(x)\mathrm{d}x\\
=&\prob{\bigcup_{n\in\N}A_n}
\end{align*}
(Summe und Integral sind hier nach dem Satz über monotone Konvergenz vertauschbar)
\end{proof}
\begin{definition}
Die Funktion $f:\R\to\R$ mit $f\geq 0$ und
\[\int_{\R}f(x)\mathrm{d}x=1\]
heißt \emph{Dichte} bezüglich des Lebesgue-Borel-Maßes vom \WM \, in Satz 4.26.
\end{definition}
\begin{bemerkung}
Ist P das zu einer Dichte $f$ gehörende \WM, so gilt für jedes $x\in\R$:
\[\prob{\{ x\}}=0\]
\end{bemerkung}
\begin{beispiel}
\begin{enumerate}[a)]
\item Die \emph{Gleichverteilung} $U(a,b)$ mit Parametern $a,b\in\R$, $a<b$ ist das durch die Dichte
\[f(x)=\begin{cases}\frac{1}{b-a}&,a\leq x\leq b\\
0&,sonst
\end{cases}\]
festgelegte \WM.

% 27.5. Timo
\item Die \emph{Exponentialverteilung} $exp(\lambda)$ mit Parameter $\lambda > 0$ ist das durch die Dichte
\[f(x)=\begin{cases}\lambda \cdot e^{-\lambda x} &,x \geq 0 \\ 0 &, x < 0\end{cases}\]
festgelegte \WM.\\
Anwendung: Modellierung von Lebensdauern/Wartezeiten\\
Es gilt: $f(x) \geq 0 \, (x \in \R)$ und
\[\int_{\R}f(x)\mathrm{d}x = \int_0^\infty \lambda \cdot e^{-\lambda x} \mathrm{d}x = \left. -e^{-\lambda x} \right|_{x=0}^\infty = 0 - (-1) = 1 \impl f(x) \text{ ist Dichte.}\]
\item Die \emph{Normalverteilung} $\m{N}(\mu, \sigma^2)$ mit Parameter $\mu \in \R$ und $\sigma > 0$ ist das durch die Dichte
\[f(x)=\frac{1}{\sqrt{2\pi} \sigma} \cdot e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\]
festgelegte \WM \, $(x \in \R)$. Es handelt sich um eine Dichte, da $f(x) \geq 0 \, (x \in \R)$ und
\[\int_{\R}f(x)\mathrm{d}x = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi} \sigma} \cdot e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \mathrm{d}x = \frac{1}{\sqrt{2 \pi}} e^{-\frac{u^2}{2}} \mathrm{d}u = 1\]
mit $u = \frac{x-\mu}{\sigma} \impl \mathrm{d}u = \frac{1}{\sigma} \mathrm{d}x$.
\end{enumerate}
\end{beispiel}
\begin{beispiel}
Die Lebensdauer einer Glühbirne in Monaten wird durch eine $exp(\frac{1}{12})$-Verteilung beschrieben. Dann tritt ein Ausfall der Glühbirne innerhalb von 36 Monaten mit folgender \Wk \, ein:
\[\p{[0,36]} = \int_0^{36} \frac{1}{12} \cdot e^{-\frac{1}{12}x} \mathrm{d}x = \left. -e^{-\frac{1}{12}x} \right|_{x=0}^{36} = -e^{-\frac{36}{12}}-(-1) \approx 0.95\]
\end{beispiel}
\subsection{Bedingte Wahrscheinlichkeiten}
Wie verändert sich das wahrscheinlichkeitstheoretische Verhalten eines \ZE s, falls eine Zusatzinformation über das Ergebnis bekannt wird?
\begin{beispiel}
Wie verändert sich die \Wk, an Hautkrebs (neu) erkrankt zu sein, falls ein Verdachtsfall beim Hautkrebsscreening auftritt? \\
Gesucht: Bedingte \Wk \, $\p{A \mid B}$ von $A$ unter der Bedingung, dass $B$ eingetreten ist ($A, B$ Ereignisse)\\
Idee: Das \ZE \, $n$-mal durchführen, seien $n_A, n_B, n_{A \cap B}$ die Anzahlen des Eintretens von $A, B, A \cap B$. Dann gilt
\[\p{A \mid B} \approx \frac{n_{A \cap B}}{n_B} = \frac{\frac{n_{A \cap B}}{n}}{\frac{n_B}{n}} \approx \frac{\p{A \cap B}}{\p{B}}\]
\end{beispiel}
\begin{definition}
Sei $(\Omega, \m{A}, P)$ ein \WR, $A,B \in \m{A}$ mit $\p{B} > 0$. Dann heißt $\p{A \mid B} = \frac{\p{A \cap B}}{\p{B}}$ \emph{bedingte \Wk \, von $A$ unter der Bedingung $B$}.
\end{definition}
\begin{lemma}
Sei $(\Omega, \m{A}, P)$ ein \WR, $A,B \in \m{A}$ mit $\p{B} > 0$. Dann ist $\tilde{P}: \m{A} \rightarrow \R, \tilde{P}(A) := \p{A \mid B}$ ein \WM. Hierbei gilt $\tilde{P}(B)=1$ (d.h. das \WM \, $\tilde{P}$ ist auf $B$ konzentriert).
\end{lemma}
\begin{proof}
Es gilt:
\begin{enumerate}[(1)]
\item $\tilde{P}(A) = \p{A \mid B} = \frac{\p{A \cap B}}{\p{B}} \geq 0$, da $P$ ein \WM \, ist
\item $\tilde{P}(\Omega) = \p{\Omega \mid B} = \frac{\p{\Omega \cap B}}{\p{B}} = \frac{\p{B}}{\p{B}} = 1$
\item Seien $A_1, A_2, \cdots \in \m{A}$ paarweise disjunkt. Dann ist
\begin{align*}
\tilde{P}(\bigcup_{n=1}^\infty A_n) &= \p{\bigcup_{n=1}^\infty A_n \mid B} = \frac{\p{(\bigcup_{n=1}^\infty A_n) \cap B}}{\p{B}} = \frac{\p{\bigcup_{n=1}^\infty (A_n \cap B)}}{\p{B}} \\
&= \frac{\sum_{n=1}^\infty \p{A_n \cap B}}{\p{B}} = \sum_{n=1}^\infty \p{A_n \mid B} = \sum_{n=1}^\infty \tilde{P}(A_n)
\end{align*}
und somit ist $\tilde{P}$ $\sigma$-additiv.
\end{enumerate}
Mit Lemma 4.14 folgt die Behauptung (1. Teil). Weiter gilt: $\tilde{P}(B) = \p{B \mid B} = \frac{\p{B \cap B}}{\p{B}} = 1$.
\end{proof}
\begin{bemerkung}
Folglich gilt z.B. $\p{A^C \mid B} = 1 - \p{A \mid B}, \p{A_1 \cup A_2 \mid B} = \p{A_1 \mid B} + \p{A_2 \mid B} - \p{A_1 \cap A_2 \mid B}.$
\end{bemerkung}
In Beispiel 4.30 gesucht: $\p{A \mid B}$ mit $A=$ \glqq Neuerkrankung Hautkrebs\grqq, $B=$ \glqq Verdachtsfall Hautkrebsscreen\grqq. Bekannt sind $\p{A} \approx 0.00328, \p{B \mid A} \approx 0.78, \p{B \mid A^C} \approx 0.26$. Wie bekommt man daraus $\p{A \mid B}$?

\begin{satz}
Sei $(\Omega, \m{A}, P)$ ein \WR, $N \in \mathbb{N}$ , $B_1 , ... , B_N \in \m{A}$ paarweise disjunkt mit $\bigcup_{n = 1}^{N}{B_n} = \Omega$ und $P(B_n) > 0 $. Dann gilt:
\begin{enumerate}[a)]
\item Für $A \in \m{A}: P(A) = \sum_{n=1}^{N}{P(A|B_n)P(B_N)}$ (Formel für totale Wahrscheinlichkeit)
\item Für $k \in \{ 1,...,N \} $ und jedes $A \in \m{A}$ mit $P(A) > 0$
\begin{align*}
P(B_K | A) = \frac{P(A|B_k)P(B_k)}{\sum_{n=1}^{N}{P(A|B_k)P(B_n)}}
\end{align*}
\end{enumerate} 
\end{satz}
\begin{proof}
\begin{enumerate}[a)]
\item Seien $B_1 \cap A,...,B_N \cap A \in \m{A}$ paarweise disjunkt mit $ \bigcup_{n=1}^{N}{(B_n \cap A)} = ( \bigcup_{n=1}^{N}{B_n}) \cap A = \Omega \cap A = A $, dann ist
\begin{align*}
P(A) &= P( \bigcup_{n=1}^{N}{(B_n \cap A)} ) = \sum_{n=1}^{N}{P(B_n \cap A)} \\
 &= \sum_{n=1}^{N}{ \frac{P(A \cap B_n)}{P(B_n)}P(B_n)} = \sum_{n=1}^{N}{P(A|B_n)P(B_n)}.
\end{align*}
\item Es gilt
\[P(B_k|A) = \frac{P(B_k \cap A)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A)},\]
woraus mit a) die Behauptung folgt.
\end{enumerate}
\end{proof}

Anwendung in Beispiel 4.30:\\
$P(A) \approx 0.00328$ \\
$P(B | A ) = 0.78 $\\
$P(B|A^c) = 0.26$
\begin{align*}
P(A|B) &= \frac{P(A \cap B)}{P(B)} \\
&= \frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^c}P(A^c) \\
&= \frac{0.78 \cdot 0.00328}{0.78 \cdot 0.00328 + 0.26 \cdot (1-0.00328)}\\
&= 0.01 
\end{align*}
\section{Zufallsvariablen und ihre Eigenschaften}
\subsection{Zufallsvariablen}
W-Raum $( \Omega , \m{A} , P)$ oft interessieren nur Teilaspekte des Ergebnisses $\omega$ eines \ZE. \\
Betrachte daher $X( \omega ) $ für ein $X : \Omega \rightarrow \Omega^\prime$ \\
\begin{beispiel}
Abstimmung über 2 Vorschläge A und B. r=3000 Personen Stimmen für A, n=1000000 Personen entscheiden sich unbeeinflusst voneinander rein zufällig. \Wk, dass A angenommen wird? \\
Modellierung: Abstimmunsverhalten der n=1000000 Personen wird beschrieben durch $\omega = ( \omega_1, ...., \omega_n) \in \{ 0,1 \}^n$ mit $\omega_i =\begin{cases}1&\text{falls i-te Person für A stimmt}\\0&\text{sonst}\end{cases}$
Wir modellieren es durch Laplaceschen W-Raum $(\Omega , \m{A} , P)$ mit $ \Omega = \{ 0,1 \}^n , \m{A} = \m{P}(\Omega) , P(A) = \frac{|A|}{| \Omega|}$ \\
Interessiert sind wir an der Anzahl Stimmen für A bei diesen n Personen, d.h. $X( \omega) = \sum_{i=1}^{n}{ \omega_i }$. \\
Frage: wie bestimmt man W-Raum, der \ZE \  mit Ergebnis $X( \omega)$ beschreibt? \\
Idee: Für Menge $A^\prime \subset \Omega^\prime$ setze $P_x(A^\prime) := P[X \in A^\prime] := P( \{ \omega \in \Omega : X( \omega ) \in A^\prime \} = P(X^{-1}(A^\prime))$ \\
Damit diese \Wk \ definiert ist muss gelten $X^{-1}(A^\prime) = \{ \omega \in \Omega : X( \omega ) \in A^\prime \}$
\end{beispiel}
\begin{definition}
$\Omega \neq \emptyset , \m{A}^\prime \sigma$-Algebra über $\Omega^\prime$. Dann heißt $(\Omega^\prime, \m{A}^\prime)$ Messraum.
\end{definition}
\begin{definition}
Sei $( \Omega , \m{A} , P)$ W-Raum , $(\Omega^\prime, \m{A}^\prime)$ Messraum. Dann heißt jede Abbildung $X: \Omega \rightarrow \Omega^\prime$ mit
\begin{align*}
 X^{-1}(A^\prime) \in \m{A} \ \forall A^\prime \in \m{A}^\prime \text{ (*) }
\end{align*} 
Zufallsvariable. \\
Im Falle $\Omega^\prime = \R, \m{A}^\prime = \m{B}$ heißt X reele \ZV.
\end{definition}
\begin{bemerkung}
\begin{itemize}
\item (*) heißt auch $ \m{A} - \m{A}^\prime$-Messbarkeit der Abbildung $X$.
\item Im Falle $\m{A} = \m{P}( \Omega ) oder A^\prime = \{ \emptyset , \Omega \}$ erfüllt jede Abbildung (*).
\item Im Falle $( \Omega , \m{A}) = ( \Omega^\prime , \m{A}^\prime) = ( \R , \m{B} ) $ kann man nur schwer Abbildungen " anschaulich konstruieren", die (*) nicht erfüllen.
\end{itemize}
\end{bemerkung}
\begin{satz}
$( \Omega , \m{A} , P)$ W-Raum , $(\Omega^\prime, \m{A}^\prime)$ Messraum, $X : \Omega \rightarrow \Omega^\prime$ \ZV. Dann ist $( \Omega^\prime , \m{A}^\prime , P_x) $ mit $ P_x(A^\prime) = P(X^{-1}(A^\prime)), ( A^\prime \in \m{A}^\prime ) $.
\end{satz}
\begin{proof}
\begin{itemize}
\item Für $ A^\prime \in \m{A}^\prime, P_X(A^\prime) \geq 0 $ da $P(A) \geq 0$
\item $P_X(\Omega^\prime ) = P(\Omega) = 1$
\item $A_1^\prime, A_2^\prime, ... \in \m{A}^\prime $ paarweise disjunkt. Wegen X \ZV gilt $X^{-1}(A_1^\prime, X^{-1}(A_2^\prime,... \in \m{A}$ paarweise disjunkt da für $ \omega \in \Omega $ gilt: 
\begin{align*}
\omega \in X^{-1}(A_i^\prime) \cap X^{-1}(A_j^\prime) \\
\Leftrightarrow \omega \in X^{-1}(A_i^\prime) \land X^{-1}(A_j^\prime) \\
\Leftrightarrow X(\omega) \in A_i^\prime \land X(\omega) \in A_j^\prime \\
\Leftrightarrow X(\omega) \in A_i^\prime \cap A_j^\prime
\end{align*} 
Mit $X^{-1}( \bigcup_{n=1}^{ \infty }{A_n^\prime} ) = \bigcup_{n=1}^{ \infty }{X^{-1}(A_n^\prime)}$ folgt
\begin{align*}
P_X(\bigcup_{n=1}^{ \infty }{A_n^\prime}) \\
&= P(X^{-1} ( \bigcup_{n=1}^{ \infty }{A_n^\prime} )) \\
&= P(\bigcup_{n=1}^{ \infty }{X^{-1}(A_n^\prime)}) \\
&= \sum_{n=1}^{\infty}{P(X^{-1}(A_n^\prime)} \\
&= \sum_n=1^\infty {P_X(A_n^\prime)}
\end{align*}
\end{itemize}
Mit Lemma 4.14 folgt die Behauptung \\
\end{proof}
Anwendung in Beispiel 5.1 \\
Wegen $\m{A} = \m{P}( \Omega) $ ist X \ZV. Vorschlag A wird anngenommen, falls gilt
\begin{align*}
r + X(\omega) > n - X( \omega ) \\
\Leftrightarrow X( \omega) > \frac{n-r}{2} \\
\Leftrightarrow X( \omega) \in \{ \frac{n-r}{2}+1,\frac{n-r}{2}+2,...,n \}
\end{align*}  
Die \Wk für die Annahme von Vorschlag A ist damit 
\begin{align*}
P[X > \frac{n-r}{2} ] \\
&= P( \{ \omega \in \Omega : X( \omega >  \frac{n-r}{2}) \\
&= P_X( \{ \frac{n-r}{2}+1,\frac{n-r}{2}+2,...,n \} ) \\
&= \sum_{k = \frac{n-r}{2}+1}^n {P_X( \{ k \} )}
\end{align*}  
Bestimmung von $P_X( \{ k \} ) = P( \{ \omega \in \Omega : \omega_1 + ... + \omega_n = k \} )$.  Diese Menge besteht aus allen n-Tupeln von Nullen und Einsen, die genau k Einsen enthalten. Da es genau  $\binom{n}{k}$   solcher n-Tupel gibt und jedes dieser n-Tupel im Laplaceschen W-Raum  $( \Omega , \m{A} , P)$ die \Wk $\frac{1}{2^n}$ hat gilt $P_X(k) = \binom{n}{k} \cdot \frac{1}{2^n}. P_X$  ist also Binomialverteilung mit Parametern n und p = 0.5. Die gesuchte \Wk ist $\sum_{k= \frac{n-r}{2} +1}^n{\binom{n}{k} \cdot \frac{1}{2^n}}$

\begin{definition}
Das W-Maß $P_X$ im Satz 5.4 heißt Verteilung der \ZV  \ X.
\end{definition}

\begin{bemerkung}
Die Begriffe W-Maß und Verteilung werden Synonym verwendet, denn: \\
Jede Verteilung ist W-Maß \\
Jedes W-Maß ist Verteilung einer \ZV, nämlich $X: \Omega \rightarrow \Omega X( \omega) = \omega $
\end{bemerkung}
Im Folgenden: Ubertragung der Bezeichnungen für W-Maße auf Verteilungen. \\
Schreibweisen : 
\begin{align*}
P[X \in A] := P( \{ \omega \in \Omega : X(\omega ) \in A \} ) \\
P[X = x] := P( \{ \omega \in \Omega : X(\omega ) = x \} )
\end{align*}

\begin{definition}
Reelle \ZV \ heißt diskrete \ZV \ , falls für abzählbar undendliche Menge $ A \subset \R $ gilt $ P_X(A) = 1 $. In diesem Fall heißt für $(x_k)_{k \in \N } \subset A $ die folge $ (P[x = x_k])_{k \in \N } $ bzw. $(P[x = x_k])_{k = 1,...,N } $ mit $P_X( \{ x_k \} ) = 1$ bzw. $P_X( \{ x_k \} k = 1,...,N ) = 1$ Zähldichte von X. \\
Und stetig verteilte \ZV \ mit Dichte $ f: \R \rightarrow \R $ falls gilt $ P_X(B) = \int_B f(x) dx ( B \in \m{B} ) $ In diesem Fall heißt f Dichte von X. 
\end{definition}

\begin{definition}
Reelle \ZV X mit $P[x = k] = \binom{n}{k} p^k (1-p)^{n-k}$ heißt binomialverteilt mit Parametern n und p kurz b(n,p)-verteilt.
\end{definition}
\begin{definition}
Reelle \ZV X mit
 \begin{align*}
f(x)=\begin{cases}\frac{1}{b-a}&,a\leq x\leq b\\
0&,sonst
\end{cases}\
\end{align*}
heißt gleichverteilt auf [a,b] order kurz $\m{U}[a,b]$-verteilt. \\
Analog für exponential-verteilt exp( $\lambda$)-verteilt \\
und für die Normalverteilung $ \m{N}( \mu , \sigma^2)$-verteilt
\end{definition}
\subsection{Unabhängigkeit}
Wann beeinflussen sich Ergebnisse von \ZE gegenseitig nicht? W-Raum $( \Omega , \m{A} , P), A,B \in \m{A} $ mit $ P(A) > 0, P(B) > 0$ \\
Idee: A,B beeinflussen sich gegenseitig nicht 
\begin{align*}
\Leftrightarrow P(A|B) = P(A) \land P(B|A) = P(B) \\
\Leftrightarrow \frac{P(A \cap B}{P(B)} = P(A) \land \frac{P(B \cap A}{P(A)} = P(B) \\
\Leftrightarrow P(A \cap B) = P(A)P(B)
\end{align*}

\begin{definition}
$( \Omega , \m{A} , P), A,B \in \m{A} $ W-Raum $ A,B \in \m{A} $ heißen unabhängig genau dann wenn $ P(A \cap B) = P(A)P(B) $
\end{definition}

\begin{beispiel}
Rein zufälliges Werfen zweier echter Würfel \\
A = " 1. Würfel 6" \\
B = " 2. Würfel 3 " \\
C = " \ Summe $\geq$ 11 \\
Dann gilt A,B unabhängig und A,C nicht unabhängig. \\

Begründung: Laplacescher W-Raum mit $ \Omega = \{ (i,j) i,j \in \{ 1,...,6 \} \}$ Dann 
\begin{align*}
A = \{ (6,1),(6,2),...,(6,6) \} \\
B = \{ (1,3),(2,3),...,(6,3) \} \\
C = \{ (6,5),(5,6),(6,6) \} \\
\Rightarrow P(A \cap B) = P( \{ (6,3) \} ) = \frac{1}{36} \\
P(A)P(B) = \frac{6}{36} \frac{6}{36} = \frac{1}{36}
\end{align*}
Also sind A,B unabhängig 
\begin{align*}
P(A \cap C ) = P(\{ (6,5),(6,6) \} ) = \frac{2}{36} \\
P(A)P(C) = \frac{6}{36} \frac{3}{36} = \frac{1}{6} \frac{1}{12} \neq \frac{2}{36} 
\end{align*}
also sind A,C nicht unabhängig \\
\end{beispiel}
Idee: \ZV  X,Y unabhängig genau dann wenn Alle Ereignisse der Form  $[x \in A] = x^{-1}(A) $ und $ [Y \in B] = Y^{-1}(B)$ sind unabhängig.

\begin{definition}
$( \Omega , \m{A} , P) $ W-Raum , $ ( \Omega_X , \m{A}_X ) , (\Omega_Y , \m{A}_Y ) $ Messräume , $X: \Omega \rightarrow \Omega_X , Y: \Omega \rightarrow \Omega_Y$ Zuffalsvariablen. 
\begin{align*}
X,Y \text{unabhängig} \Leftrightarrow \forall A \in \m{A}_X , B \in \m{A}_Y = P[x \in A]P[Y \in B]
\end{align*}
\end{definition}

\begin{bemerkung}
\begin{itemize}
\item $ P[X \in A , Y \in B] = P(X^{-1}(A) \cap Y^{-1}(B)) $ \\
 und $ P[X \in A]P[ Y \in B ] = P(X^{-1}(A))P(Y^{-1}(B)) $
\item Um von Unabhängigkeit bei Zufallsvariablen sprechen zu können müssen diese auf dem gleichen W-Raum definiert sein.
\item Anschaulich: Werte unabhängiger Zufallsvariablen beeinflussen sich gegenseitig nicht.
\item Zusammenhang mit Definition 5.9: Zwei Ereignisse A,B sind unabhängig genau dann wenn $1_A, 1_b$ unabhängig. 
\end{itemize}
\end{bemerkung}
\begin{beispiel}
Werfen zweier echter Würfel \\
X = " \  Augenzahl 1. Würfel " \\
Y = " \  Augenzahl 2. Würfel " \\
Z = " \  Summe Augenzahl " \\
Dann gilt a) X,Y unabhängig und b) X,Z nicht unabhängig \\
Begründung: b) Wie in bsp. 5.10 folgt $P[ X \in \{ 6 \} , Z \in \{ 11 , 12 \} ] \neq P[X \in \{ 6 \} ]P[Z \in \{ 11 , 12 \} ]$ \\
a) Laplacescher W-Raum mit $ \Omega \{ ( i,j) i,j \in \{ 1,..,6 \} \} , X,Y ; \Omega \rightarrow \{ 1,..,6 \} , \m{A}_X = \m{A}_Y = P( \{ 1,...,6 \} ) , X((i,j)) = i , Y((i,j))=j$ \\
Dann gilt für A,B $\leq \{1,...,6 \} $
\begin{align*}
P[X \in A , Y \in B] \\
&= P( \{ (i,j) \in \Omega : i \in A , j \in B \} ) \\
&= P( A \times B ) \\
&= \frac{| A \times B |}{| \Omega  |} = \frac{|A| |B|}{36} = \frac{|A|}{6} \frac{|B|}{6}
\end{align*} 
sowie 
\begin{align*}
P[X \in A] \\
&= P( \{ (i,j) \in \Omega : i \in A ) \\
&= P( A \times \{ 1,...,6 \} ) \\
&= \frac{|A| 6}{36} \\
&= \frac{|A|}{6}
\end{align*}
und analog $P[Y \in B] = \frac{|B|}{6}$. Also sind X und Y unabhängig.
\end{beispiel}

% 10.06. Timo
\begin{definition}
Sei $(\Omega, \m{A}, P)$ ein \WR \, und seien $(\Omega, \m{A}_i) \, (i \in \N)$ Messräume sowie $X_i: \Omega \rightarrow  \Omega \, (i \in \N)$ Zufallsvariablen.
\begin{enumerate}[a)]
\item $X_1, \ldots , X_n$ heißen genau dann \emph{unabhängig}, wenn $P[X_1 \in A_1, \ldots , X_n \in A_n] = P[X_1 \in A_1] \cdot \ldots \cdot P[X_n \in A_n]$ für alle $A_1 \in \m{A}_1, \ldots , A_n \in \m{A}_n$ gilt.
\item $(X_n)_n$ ist \emph{unabhängig}, wenn $X_1, \ldots , X_n$ für alle $n \in \N$ unabhängig sind.
\end{enumerate}
\end{definition}
\begin{lemma}
Seien $X_1, \ldots , X_n$ unabhängige reelle Zufallsvariablen definiert auf $(\Omega, \m{A}, P)$. Für $h_1, \ldots , h_n : \R \rightarrow \R$ mit
\[h_i^{-1}(B) \in \m{B} \text{ für alle } B \in \m{B} \, (i=1, \ldots , n)\]
gilt dann: Die Zahlen
\[h_1 \circ X_1, \ldots , h_n \circ X_n\]
sind unabhängig.
\end{lemma}
\begin{proof}
Für $B \in \m{B}$ gilt:
\[(h_i \circ X_i)^{-1}(B) = X_i^{-1}(h_i^{-1}(B)) \in \m{A},\]
da $X_i$ reelle Zufallsvariablen sind und $h_i^{-1}(B) \in \m{B}$ nach Voraussetzung ist. Also sind $h_i \circ X_i$ reelle Zufallsvariablen. Weiter gilt für $B_1, \ldots , B_n \in \m{B}$:
\begin{align*}
&P[h_1 \circ X_1 \in B_1, \ldots , h_n \circ X_n \in B_n] \\
=&P(\{\omega \in \Omega : (h_1 \circ X_1)(\omega) \in B_1, \ldots , (h_n \circ X_n)(\omega) \in B_n\})\\
=&P(\{\omega \in \Omega : X_1(\omega) \in h_1^{-1}(B_1), \ldots ,X_n(\omega) \in h_n^{-1}(B_n)\})\\
=&P[X_1 \in h_1^{-1}(B_1), \ldots, X_n \in h_n^{-1}(B_n)]\\
=&P[X_1 \in h_1^{-1}(B_1)] \cdot \ldots \cdot P[X_n \in h_n^{-1}(B_n)]\\
=&P[h_1 \circ X_1 \in B_1] \cdot \ldots \cdot P[h_n \circ X_n \in B_n]
\end{align*}
\end{proof}

\textbf{Folgerung:} Mit $X, Y$ sind z.B. auch $X^2, exp(Y)$ unabhängig.\\
\textbf{Erweiterung:} Mit $X_1, \ldots , X_4$ sind auch $X_1 \cdot X_2, exp(X_3 + X_4)$ unabhängig (vgl. Wahrscheinlichkeitstheorie).

\subsection{Der Erwartungswert}

Ziel: Definition des \glqq mittleren Wertes\grqq \, eines Zufallsexperiments mit reellen Ergebnissen.
\begin{beispiel}
Es gibt zehn Enten und zehn perfekte Schützen. Jeder Schütze wählt unbeeinflusst von den anderen rein zufällig eine Ente aus, auf die er schießt. Wie viele Enten überleben im Mittel? \\
$X=$ zufällige Zahl überlebender Enten, d.h.
\[X(\omega) \in \{0,1, \ldots, 9\}.\]
Gesucht ist ein \glqq mittlerer Wert\grqq \, $EX$ (sog. \emph{Erwartungswert}) von $X$. Wir setzen
\[EX = \int_\Omega \m{X} \mathrm{d}P\]
als das sog. Maßintegral.
\end{beispiel}
\begin{definition}
Wir setzen $\bar{\R} = \R \cup \{\infty, - \infty\}$ mit Rechenregeln
\begin{align*}
&a + \infty = \infty = \infty + a, \\
&a - \infty = - \infty = - \infty + a, \\
&\infty + \infty = \infty, \\
&- \infty - \infty = - \infty
\end{align*}
für $a \in \R$ und
\[b \cdot \infty = \infty \cdot b = \infty\]
für $b \in \R_+ \backslash \{0\}$ und
\begin{align*}
&\infty \cdot \infty = \infty, \\
&\infty \cdot 0 = 0 = 0 \cdot \infty
\end{align*}
\end{definition}
\begin{bemerkung}
Nicht definiert sind z.B. $\infty - \infty, \frac{\infty}{\infty}.$
\end{bemerkung}
\begin{definition}
Sei $\Omega \neq \emptyset, \m{A} \, \sigma$-Algebra über $\Omega$. Eine Funktion $\mu: \m{A} \rightarrow \bar{\R}$ heißt Maß, falls folgende Eigenschaften gelten:
\begin{enumerate}[(i)]
\item $\mu(A) \geq 0 \, (A \in \m{A})$
\item $\mu(\emptyset) = 0$
\item $A, B \in \m{A}$ mit $A \subseteq B$: $\mu(A) \leq \mu(B)$
\item $A, B \in \m{A}$ mit $A \cap B = \emptyset \impl \mu(A \cap B) = \mu(A) + \mu(B)$
\item $A_1, \ldots , A_n \in \m{A}$ paarweise disjunkt $\impl \mu(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n \mu(A_i)$
\item $A_1, A_2, \ldots \in \m{A}$ paarweise disjunkt $\impl \mu(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \mu(A_i)$
\end{enumerate}
In diesem Fall heißt $(\Omega, \m{A}, \mu)$ \emph{Maßraum}.
\end{definition}
\begin{bemerkung}
$P$ ist ein \WM \, $\Leftrightarrow P$ ist ein Maß und $P(\Omega)=1$.
\end{bemerkung}
\begin{definition}
Sei $(\Omega, \m{A})$ ein Messraum. Eine Funktion $f: \Omega \rightarrow \R$ heißt $\m{A}$-$\m{B}$-messbar (kurz: messbar), wenn
\[f^{-1}(B) \in \m{A} \text{ für alle } B \in \m{B}\]
gilt.
\end{definition}
\begin{bemerkung}
Sei $(\Omega, \m{A}, P)$ ein \WR, dann ist jede messbare Abbildung $X: \Omega \rightarrow \R$ eine reelle Zufallsvariable.
\end{bemerkung}
Im Folgenden definieren wir ein Integral 
\[\int_\Omega h \mathrm{d}\mu\]
für den Maßraum $(\Omega, \m{A}, \mu)$ und eine messbare Funktion $h: \Omega \rightarrow \R$.\\
Die Definition erfolgt in drei Schritten:
\begin{definition}
Sei $(\Omega, \m{A})$ ein Messraum. Jede Funktion
\[h = \sum_{i=1}^n \alpha_i \cdot \m{X}_{A_i}\]
mit $n \in \N, A_1, \ldots, A_n \in \m{A}$ Partition von $\Omega$ und $\alpha_1, \ldots, \alpha_n \in \R$ heißt \emph{einfache Funktion}.\\
\end{definition}
Schritt 1: Ist $h = \sum_{i=1}^n \alpha_i \cdot \m{X}_{A_i}$ eine nichtnegative einfache Funktion, so setzen wir
\[\int_\Omega h \mathrm{d}\mu = \sum_{i=1}^n \alpha_i \cdot \mu(A_i).\]

% 13.06. Timo
\begin{bemerkung}
Das Integral in Schritt 1 ist wohldefiniert:
\begin{enumerate}[a)]
\item Wegen $\alpha_i \geq 0$ tritt in der Summe nicht der Fall $\infty - \infty$ auf.
\item Ist
\[\sum_{i=1}^n \alpha_i \cdot \m{X}_{A_i} = \sum_{j=1}^m \beta_j \cdot \m{X}_{B_j}\]
mit $n, m \in \N$, $\alpha_1 , \ldots , \alpha_n, \beta_1 , \ldots , \beta_n \in \R$, $A_1, \ldots , A_n \in \m{A}$ und $B_1, \ldots , B_m \in \m{A}$ Partition von $\Omega$, so gilt
\[\sum_{i=1}^n \alpha_i \cdot \mu(A_i) = \sum_{j=1}^m \beta_j \cdot \mu(B_j).\]
\end{enumerate}
\end{bemerkung}
\begin{proof}
Wegen $B_1, \ldots , B_m \in \m{A}$ Partition von $\Omega$ ist
\[A_i \cap B_1 , \ldots , A_i \cap B_m \in \m{A}\]
eine Partition von $A_i$. Da $\mu$ ein \WM \, ist, gilt
\begin{align*}
&\mu(A_i) = \sum_{j=1}^m \mu(A_i \cap B_j)\\
\impl & \sum_{i=1}^n \alpha_i \cdot \mu(A_i) = \sum_{i=1}^n \sum_{j=1}^m \alpha_i \cdot \mu(A_i \cap B_j).
\end{align*}
Analog gilt:
\begin{align*}
\sum_{j=1}^m \beta_j \cdot \mu(B_j) &= \sum_{j=1}^m \sum_{i=1}^n \beta_j \cdot \mu(B_j \cap A_i)\\
&= \sum_{i=1}^n \sum_{j=1}^m \beta_j \cdot \mu(A_i \cap B_j).
\end{align*}
Im Fall $\mu(A_{i_0} \cap B_{j_0}) \neq \emptyset$ existiert ein $\omega \in A_{i_0} \cap B_{j_0}$, woraus folgt:
\[\alpha_{i_0} = \sum_{i=1}^n \alpha_i \cdot \m{X}_{A_i}(\omega) = \sum_{j=1}^m \beta_j \cdot \m{X}_{B_j}(\omega) = \beta_{j_0},\]
da $\omega \in A_{i_0}$ und $\omega \in B_{i_0}$. Daraus folgt
\[\alpha_{i_0} \cdot \mu(A_{i_0} \cap B_{j_0}) = B_{j_0} \cdot \mu(A_{i_0} \cap B_{j_0}).\]
Letzteres gilt auch für $\mu(A_{i_0} \cap B_{j_0}) = 0$. Damit folgt die Behauptung.
\end{proof}
\begin{definition}
Eine Folge $(f_n)_n$ von Funktionen $f_n: \Omega \rightarrow \R$ \emph{konvergiert von unten} gegen $f: \Omega \rightarrow \R$, falls gilt:
\[f_1(\omega) \leq f_2(\omega) \leq \ldots \text{ und } \lim_{n \rightarrow \infty} f_n(\omega) = f(\omega)\]
für alle $\omega \in \Omega$. Schreibweise: $f_n \uparrow f$.
\end{definition}
\begin{bemerkung}
Man kann zeigen: Zu jeder nichtnegativen messbaren Funktion $f: \Omega \rightarrow \R$ existieren nichtnegative einfache Funktionen $f_n: \Omega \rightarrow \R$ mit $f_n \uparrow f$.
\end{bemerkung}
Schritt 2: Ist $f: \Omega \rightarrow \R$ nichtnegativ und messbar, so wählen wir nichtnegative einfache Funktionen $f_n$ mit $f_n \uparrow f$ und setzen
\[\int_\Omega f \mathrm{d}\mu = \lim_{n \rightarrow \infty} \int_\Omega f_n \mathrm{d}\mu.\]
\begin{bemerkung}
Man kann zeigen: Der Grenzwert oben existiert und hängt nicht von der Wahl der $f_n$ mit $f_n \uparrow f$ ab.
\end{bemerkung}
Schritt 3: Nimmt die messbare Funktion $f: \Omega \rightarrow \R$ auch negative Werte an, so zerlegen wir $f$ gemäß
\[f = f^+ - f^-\]
mit
\begin{align*}
&f^+(\omega) = max\{f(\omega), 0\} \\&f^-(\omega) = max\{-f(\omega), 0\}
\end{align*}
in zwei nichtnegative messbare Funktionen $f^+$ und $f^-$ und setzen im Falle
\begin{align*}
&\int_\Omega f^+ \mathrm{d}\mu < \infty \text{ oder } \int_\Omega f^- \mathrm{d}\mu < \infty: \\
&\int_\Omega f \mathrm{d}\mu = \int_\Omega f^+ \mathrm{d}\mu - \int_\Omega f^- \mathrm{d}\mu.
\end{align*}
\begin{bemerkung}
Obiges $f^+$ ist als Verkettung der Abbildung $u \rightarrow max\{u, 0\}$ (messbar, da stetig) und der messbaren Abbildung $f$ selbst messbar. Analog gilt das auch für $f^-$. Schreibweisen:
\[\int_\Omega f \mathrm{d}\mu = \int f \mathrm{d}\mu = \int_\Omega f(\omega) \mu(d\omega) = \int f(\omega) \mu(d\omega)\]
\end{bemerkung}
\begin{definition}
Sei $(\Omega, \m{A}, P)$ ein \WR \, und $X: \Omega \rightarrow \R$ eine reelle \ZV. Dann heißt
\[EX := \int_\Omega X \mathrm{d}P\]
(sofern existent) \emph{Erwartungswert} von $X$.
\end{definition}
In Beispiel 5.15 gilt für die Zahl $X$ der überlebenden Enten:
\[X(\omega) = \sum_{k=0}^9 k \cdot \m{X}_{\{\bar{\omega} \in \Omega : X(\bar{\omega}) = k\}}(\omega),\]
d.h. $X$ ist eine nichtnegative einfache \ZV. Damit:
\begin{align*}
EX &= \int X \mathrm{d}P \\
&= \sum_{k=0}^9 k \cdot P(\{\bar{\omega} \in \Omega : X(\bar{\omega}) = k\}\\
&= \sum_{k=0}^9 k \cdot P[X=k].
\end{align*}
\begin{bemerkung}
Diese Definition von $EX$ als \glqq Mittelwert\grqq \, ist plausibel, denn: \\
Sind $x_1, \ldots , x_n$ Werte von $X$ bei $n$-maliger unbeeinflusster Wiederholung des \ZE s, so gilt
\begin{align*}
\text{\glqq Mittelwert\grqq \,} &\approx \frac{1}{n} \sum_{i=1}^n x_i = \frac{1}{n} \sum_{k=0}^9 k \cdot \# \{1 \leq i \leq n : x_i = k\} \\
&= \sum_{k=0}^9 k \cdot \frac{\# \{1 \leq i \leq n : x_i = k\}}{n} \\
&\approx \sum_{k=0}^9 k \cdot P[X=k].
\end{align*}
\end{bemerkung}
\end{document}
